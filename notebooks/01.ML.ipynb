{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e1d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.7.0+cu126\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde461e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Score</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reliable application</td>\n",
       "      <td>0.205</td>\n",
       "      <td>Reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>issue alerts</td>\n",
       "      <td>0.205</td>\n",
       "      <td>Reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low complexity</td>\n",
       "      <td>0.205</td>\n",
       "      <td>Reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast algorithms</td>\n",
       "      <td>0.205</td>\n",
       "      <td>Reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>0.205</td>\n",
       "      <td>Reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>High-security safety connection</td>\n",
       "      <td>0.855</td>\n",
       "      <td>Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Comprehensive resource segmentation</td>\n",
       "      <td>0.855</td>\n",
       "      <td>Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Full device trust validation</td>\n",
       "      <td>0.855</td>\n",
       "      <td>Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Advanced asset segmentation</td>\n",
       "      <td>0.855</td>\n",
       "      <td>Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Advanced encrypted connection</td>\n",
       "      <td>0.855</td>\n",
       "      <td>Safety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Keywords  Score        Class\n",
       "0                   reliable application  0.205  Reliability\n",
       "1                           issue alerts  0.205  Reliability\n",
       "2                         low complexity  0.205  Reliability\n",
       "3                        fast algorithms  0.205  Reliability\n",
       "4                       machine learning  0.205  Reliability\n",
       "..                                   ...    ...          ...\n",
       "137      High-security safety connection  0.855       Safety\n",
       "138  Comprehensive resource segmentation  0.855       Safety\n",
       "139         Full device trust validation  0.855       Safety\n",
       "140          Advanced asset segmentation  0.855       Safety\n",
       "141        Advanced encrypted connection  0.855       Safety\n",
       "\n",
       "[142 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"data/dataset.csv\")\n",
    "data_df['Score'] = data_df['Score'].apply(lambda x: sum(map(float, x.split(','))) / 2 / 100)  # Normalize score\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabe7909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability\n",
      "           Score\n",
      "count  32.000000\n",
      "mean    0.561250\n",
      "std     0.289535\n",
      "min     0.205000\n",
      "25%     0.205000\n",
      "50%     0.605000\n",
      "75%     0.905000\n",
      "max     0.905000\n",
      "[0.205 0.605 0.905]\n",
      "\n",
      "\n",
      "Privacy\n",
      "           Score\n",
      "count  30.000000\n",
      "mean    0.638333\n",
      "std     0.253708\n",
      "min     0.305000\n",
      "25%     0.305000\n",
      "50%     0.705000\n",
      "75%     0.905000\n",
      "max     0.905000\n",
      "[0.305 0.705 0.905]\n",
      "\n",
      "\n",
      "Security\n",
      "           Score\n",
      "count  30.000000\n",
      "mean    0.763333\n",
      "std     0.102076\n",
      "min     0.550000\n",
      "25%     0.681250\n",
      "50%     0.762500\n",
      "75%     0.825000\n",
      "max     0.950000\n",
      "[0.9   0.75  0.825 0.675 0.775 0.875 0.65  0.95  0.725 0.625 0.7   0.85\n",
      " 0.55 ]\n",
      "\n",
      "\n",
      "Resilience\n",
      "           Score\n",
      "count  21.000000\n",
      "mean    0.584762\n",
      "std     0.278368\n",
      "min     0.170000\n",
      "25%     0.505000\n",
      "50%     0.505000\n",
      "75%     0.840000\n",
      "max     0.840000\n",
      "[0.17  0.505 0.84 ]\n",
      "\n",
      "\n",
      "Safety\n",
      "           Score\n",
      "count  29.000000\n",
      "mean    0.515345\n",
      "std     0.274636\n",
      "min     0.205000\n",
      "25%     0.205000\n",
      "50%     0.555000\n",
      "75%     0.855000\n",
      "max     0.855000\n",
      "[0.205 0.555 0.855]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tf in data_df[\"Class\"].unique():\n",
    "    print(tf)\n",
    "    tf_df = data_df[data_df[\"Class\"] == tf]\n",
    "    print(tf_df.describe())\n",
    "    print(tf_df[\"Score\"].unique())\n",
    "\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c6d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into train and validation sets\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ab706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bilito/Documents/FRONT_RG/coco/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTForQuantification(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased'):\n",
    "        super(BERTForQuantification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Separate heads for each class (Reliability, Privacy, Security)\n",
    "        self.reliability_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.privacy_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.security_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.resilience_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.safety_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, class_type):\n",
    "        # BERT pooled output\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Use the correct regression head based on class_type\n",
    "        if class_type == \"Reliability\":\n",
    "            score = self.reliability_head(pooled_output)\n",
    "        elif class_type == \"Privacy\":\n",
    "            score = self.privacy_head(pooled_output)\n",
    "        elif class_type == \"Security\":\n",
    "            score = self.security_head(pooled_output)\n",
    "        elif class_type == \"Resilience\":\n",
    "            score = self.resilience_head(pooled_output)\n",
    "        elif class_type == \"Safety\":\n",
    "            score = self.safety_head(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid class_type. Must be one of: Reliability, Privacy, Security.\")\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import os\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "        save_dir,\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        early_stopping_patience=3,\n",
    "        max_grad_norm=1.0\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "\n",
    "    def train(self, epochs):\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            all_targets = []\n",
    "            all_predictions = []\n",
    "\n",
    "            for batch in tqdm(self.train_dataloader, desc=\"Training\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                score = batch['score'].to(self.device)\n",
    "                class_type = batch['class_type']\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = []\n",
    "                for i in range(len(input_ids)):\n",
    "                    output = self.model(\n",
    "                        input_ids[i].unsqueeze(0),\n",
    "                        attention_mask[i].unsqueeze(0),\n",
    "                        class_type[i]\n",
    "                    )\n",
    "                    outputs.append(output)\n",
    "\n",
    "                outputs = torch.cat(outputs).squeeze(1)\n",
    "                loss = self.criterion(outputs, score)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                all_predictions.extend(outputs.detach().cpu().numpy())\n",
    "                all_targets.extend(score.cpu().numpy())\n",
    "\n",
    "            # Training metrics\n",
    "            mae, mse, rmse, r2, mape = self.calculate_metrics(all_targets, all_predictions)\n",
    "            avg_train_loss = running_loss / len(self.train_dataloader)\n",
    "            print(f\"Training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Training Metrics - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, MAPE: {mape:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            val_loss = self.validate(epoch)\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\"✅ Validation loss improved. Saving model.\")\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                self.save_model(epoch)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"⚠️ Validation loss did not improve. Patience: {patience_counter}/{self.early_stopping_patience}\")\n",
    "                if patience_counter >= self.early_stopping_patience:\n",
    "                    print(\"⏹️ Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dataloader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                score = batch['score'].to(self.device)\n",
    "                class_type = batch['class_type']\n",
    "\n",
    "                outputs = []\n",
    "                for i in range(len(input_ids)):\n",
    "                    output = self.model(\n",
    "                        input_ids[i].unsqueeze(0),\n",
    "                        attention_mask[i].unsqueeze(0),\n",
    "                        class_type[i]\n",
    "                    )\n",
    "                    outputs.append(output)\n",
    "\n",
    "                outputs = torch.cat(outputs).squeeze(1)\n",
    "                val_loss = self.criterion(outputs, score)\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "                all_predictions.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(score.cpu().numpy())\n",
    "\n",
    "        mae, mse, rmse, r2, mape = self.calculate_metrics(all_targets, all_predictions)\n",
    "        avg_val_loss = running_val_loss / len(self.val_dataloader)\n",
    "\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Metrics - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, MAPE: {mape:.4f}\")\n",
    "\n",
    "        self.final_val_loss = avg_val_loss\n",
    "        return avg_val_loss\n",
    "\n",
    "    def calculate_metrics(self, targets, predictions):\n",
    "        mae = mean_absolute_error(targets, predictions)\n",
    "        mse = ((torch.tensor(targets) - torch.tensor(predictions)) ** 2).mean().item()\n",
    "        rmse = mse ** 0.5\n",
    "        r2 = r2_score(targets, predictions)\n",
    "        mape = torch.mean(\n",
    "            torch.abs((torch.tensor(targets) - torch.tensor(predictions)) / torch.tensor(targets))\n",
    "        ).item() * 100\n",
    "        return mae, mse, rmse, r2, mape\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        save_path = os.path.join(self.save_dir, f\"best_model.pth\")\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "\n",
    "    def get_validation_loss(self):\n",
    "        return getattr(self, \"final_val_loss\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3032ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bilito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def synonym_replacement(text, num_replacements=1):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    candidates = [i for i, w in enumerate(words) if len(wordnet.synsets(w)) > 0]\n",
    "\n",
    "    if not candidates:\n",
    "        return text\n",
    "\n",
    "    random.shuffle(candidates)\n",
    "    n_replacements = min(num_replacements, len(candidates))\n",
    "\n",
    "    for i in candidates[:n_replacements]:\n",
    "        synonyms = wordnet.synsets(words[i])\n",
    "        lemmas = [l.name().replace(\"_\", \" \") for s in synonyms for l in s.lemmas()]\n",
    "        lemmas = list(set([l for l in lemmas if l.lower() != words[i].lower()]))\n",
    "        if lemmas:\n",
    "            new_words[i] = random.choice(lemmas)\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b501901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Define augmentation techniques\n",
    "def synonym_replacement(text, n=2):\n",
    "    \"\"\"\n",
    "    Replace n random words in the text with their synonyms.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for word in random_word_list:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            if synonym != word:  # Ensure synonym is different\n",
    "                new_words = [synonym if w == word else w for w in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= n:\n",
    "                break\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def augment_text(text):\n",
    "    \"\"\"\n",
    "    Apply a random augmentation technique to the input text.\n",
    "    \"\"\"\n",
    "    techniques = [synonym_replacement]  # Add more techniques here if needed\n",
    "    augmentation = random.choice(techniques)\n",
    "    return augmentation(text)\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        text = sample['Keywords']  # Text input\n",
    "        class_type = sample['Class']  # Class type: Reliability, Privacy, etc.\n",
    "        score = sample['Score']  # The actual score as the target\n",
    "\n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        tokens = self.tokenizer(text, padding='max_length', max_length=self.max_len, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Return the tokenized inputs and the corresponding score and class\n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'class_type': class_type,\n",
    "            'score': torch.tensor(score, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def load_data_from_csv(file_path, n_augmentations=3):\n",
    "    \"\"\"\n",
    "    Load dataset from CSV and augment the training data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Normalize scores between 0 and 1 if needed\n",
    "    df['Score'] = df['Score'].apply(lambda x: sum(map(float, x.split(','))) / 2 / 100)  # Normalize score\n",
    "    \n",
    "    # Apply data augmentation\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        original_text = row['Keywords']\n",
    "        for _ in range(n_augmentations):\n",
    "            augmented_text = augment_text(original_text)\n",
    "            augmented_rows.append({'Keywords': augmented_text, 'Score': row['Score'], 'Class': row['Class']})\n",
    "    \n",
    "    # Append augmented rows to the original dataset\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset(file_path, batch_size=16, max_len=128, augment=True):\n",
    "    \"\"\"\n",
    "    Loads the dataset from CSV, tokenizes the inputs, and returns DataLoader for training and validation.\n",
    "    \"\"\"\n",
    "    # Load and optionally augment the data\n",
    "    df = load_data_from_csv(file_path) if augment else pd.read_csv(file_path)\n",
    "    data = df.to_dict(orient='records')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    dataset = TextDataset(data, tokenizer, max_len=max_len)\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # DataLoader objects\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511a3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2399\n",
      "Training Metrics - MAE: 0.4079, MSE: 0.2468, RMSE: 0.4968, R²: -2.6437, MAPE: 73.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0793\n",
      "Validation Metrics - MAE: 0.2295, MSE: 0.0792, RMSE: 0.2814, R²: -0.2424, MAPE: 60.9207\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_1.pth\n",
      "\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0646\n",
      "Training Metrics - MAE: 0.2088, MSE: 0.0665, RMSE: 0.2579, R²: 0.0183, MAPE: 54.7371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0470\n",
      "Validation Metrics - MAE: 0.1710, MSE: 0.0462, RMSE: 0.2150, R²: 0.2747, MAPE: 44.0199\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_2.pth\n",
      "\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0365\n",
      "Training Metrics - MAE: 0.1526, MSE: 0.0368, RMSE: 0.1919, R²: 0.4561, MAPE: 40.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0346\n",
      "Validation Metrics - MAE: 0.1429, MSE: 0.0339, RMSE: 0.1842, R²: 0.4679, MAPE: 37.3584\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_3.pth\n",
      "\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0214\n",
      "Training Metrics - MAE: 0.1178, MSE: 0.0221, RMSE: 0.1486, R²: 0.6741, MAPE: 29.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0228\n",
      "Validation Metrics - MAE: 0.1167, MSE: 0.0234, RMSE: 0.1529, R²: 0.6334, MAPE: 24.0660\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_4.pth\n",
      "\n",
      "Epoch 5/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0150\n",
      "Training Metrics - MAE: 0.0923, MSE: 0.0156, RMSE: 0.1248, R²: 0.7701, MAPE: 20.3929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0159\n",
      "Validation Metrics - MAE: 0.1012, MSE: 0.0166, RMSE: 0.1287, R²: 0.7402, MAPE: 22.6432\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_5.pth\n",
      "\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0126\n",
      "Training Metrics - MAE: 0.0821, MSE: 0.0126, RMSE: 0.1122, R²: 0.8143, MAPE: 19.4923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0122\n",
      "Validation Metrics - MAE: 0.0848, MSE: 0.0127, RMSE: 0.1128, R²: 0.8006, MAPE: 18.5505\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_6.pth\n",
      "\n",
      "Epoch 7/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0108\n",
      "Training Metrics - MAE: 0.0764, MSE: 0.0107, RMSE: 0.1032, R²: 0.8427, MAPE: 17.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0187\n",
      "Validation Metrics - MAE: 0.0978, MSE: 0.0187, RMSE: 0.1368, R²: 0.7064, MAPE: 20.0636\n",
      "⚠️ Validation loss did not improve. Patience: 1/3\n",
      "\n",
      "Epoch 8/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:14<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0082\n",
      "Training Metrics - MAE: 0.0671, MSE: 0.0083, RMSE: 0.0910, R²: 0.8777, MAPE: 14.5114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0125\n",
      "Validation Metrics - MAE: 0.0789, MSE: 0.0132, RMSE: 0.1148, R²: 0.7933, MAPE: 15.1694\n",
      "⚠️ Validation loss did not improve. Patience: 2/3\n",
      "\n",
      "Epoch 9/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0056\n",
      "Training Metrics - MAE: 0.0589, MSE: 0.0059, RMSE: 0.0770, R²: 0.9124, MAPE: 12.8349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0116\n",
      "Validation Metrics - MAE: 0.0746, MSE: 0.0123, RMSE: 0.1111, R²: 0.8064, MAPE: 14.2923\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_9.pth\n",
      "\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0045\n",
      "Training Metrics - MAE: 0.0502, MSE: 0.0045, RMSE: 0.0667, R²: 0.9342, MAPE: 10.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0106\n",
      "Validation Metrics - MAE: 0.0724, MSE: 0.0113, RMSE: 0.1062, R²: 0.8231, MAPE: 13.9320\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_10.pth\n",
      "\n",
      "Epoch 11/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0041\n",
      "Training Metrics - MAE: 0.0505, MSE: 0.0042, RMSE: 0.0645, R²: 0.9386, MAPE: 11.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0098\n",
      "Validation Metrics - MAE: 0.0695, MSE: 0.0104, RMSE: 0.1020, R²: 0.8367, MAPE: 12.6959\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_11.pth\n",
      "\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0041\n",
      "Training Metrics - MAE: 0.0475, MSE: 0.0037, RMSE: 0.0612, R²: 0.9446, MAPE: 10.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0085\n",
      "Validation Metrics - MAE: 0.0611, MSE: 0.0089, RMSE: 0.0944, R²: 0.8602, MAPE: 11.7142\n",
      "✅ Validation loss improved. Saving model.\n",
      "Model saved at ./models/bert_model_epoch_12.pth\n",
      "\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0029\n",
      "Training Metrics - MAE: 0.0423, MSE: 0.0030, RMSE: 0.0546, R²: 0.9561, MAPE: 9.2908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0099\n",
      "Validation Metrics - MAE: 0.0672, MSE: 0.0105, RMSE: 0.1024, R²: 0.8355, MAPE: 12.6180\n",
      "⚠️ Validation loss did not improve. Patience: 1/3\n",
      "\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0027\n",
      "Training Metrics - MAE: 0.0400, MSE: 0.0027, RMSE: 0.0517, R²: 0.9605, MAPE: 9.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0097\n",
      "Validation Metrics - MAE: 0.0650, MSE: 0.0104, RMSE: 0.1021, R²: 0.8363, MAPE: 11.7729\n",
      "⚠️ Validation loss did not improve. Patience: 2/3\n",
      "\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0025\n",
      "Training Metrics - MAE: 0.0397, MSE: 0.0027, RMSE: 0.0515, R²: 0.9608, MAPE: 8.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0086\n",
      "Validation Metrics - MAE: 0.0618, MSE: 0.0093, RMSE: 0.0966, R²: 0.8537, MAPE: 11.5974\n",
      "⚠️ Validation loss did not improve. Patience: 3/3\n",
      "⏹️ Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from CSV file\n",
    "train_dataloader, val_dataloader = load_dataset(file_path='./data/dataset.csv', batch_size=32, augment=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = BERTForQuantification()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    save_dir=\"./models\"\n",
    ")\n",
    "\n",
    "trainer.train(epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884102e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
